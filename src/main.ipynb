{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet  : Find in translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### chaque jour des dizaines d'objets sont perdus et retrouvés dans les gares parisiennes par les voyageurs, leur gestion est critique au niveau de la satisfaction client. Cependant le cout de leur gestion est critique également. On aimerait donc dimensionner au mieux le service en charge de les gérer mais pour cela il faut pouvoir anticiper de manière précise le volume d'objets trouvés chaque jour. Votre manager a une intuition qu'il aimerait vérifier: plus il fait froid plus les voyageurs sont chargés (manteau, écharppes, gant) plus ils ont donc de probabilité de les oublier. Mais empiler toutes ces couches prend du temps, ce qui pousse aussi à se mettre en retard et dans la précipitation, à oublier d'autres affaires encore. A l'aide des données de la SNCF et d'autres données, essayez de creuser cette piste."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "connexion = sqlite3.connect('base.db')\n",
    "curseur = connexion.cursor()\n",
    "\n",
    "# Création de la table Températures\n",
    "curseur.execute(\"\"\" CREATE TABLE Temperatures(\n",
    "                    date TEXT PRIMARY KEY UNIQUE, \n",
    "                    temperature_moyenne INTEGER\n",
    "                )\n",
    "                \"\"\")\n",
    "\n",
    "# Création de la table Gares\n",
    "curseur.execute(\"\"\" CREATE TABLE Gares(\n",
    "                    code_uic INTEGER,\n",
    "                    nom_gare TEXT,\n",
    "                    latitude REAL,\n",
    "                    longitude REAL,\n",
    "                    frequentation_2019 INTEGER,\n",
    "                    frequentation_2020 INTEGER,\n",
    "                    frequentation_2021 INTEGER\n",
    "                )\n",
    "                \"\"\")\n",
    "\n",
    "# Création de la table objets trouvés\n",
    "curseur.execute(\"\"\" CREATE TABLE Objets_trouves (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "                    date TEXT,\n",
    "                    type TEXT,\n",
    "                    gare TEXT,\n",
    "                    code UIC INTEGER,\n",
    "                    FOREIGN KEY (gare) REFERENCES Gares(nom_gare),\n",
    "                    FOREIGN KEY (date) REFERENCES Temperatures(date)\n",
    "                    )\"\"\")\n",
    "\n",
    "\n",
    "connexion.commit()\n",
    "connexion.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "connexion = connexion = sqlite3.connect('base.db')\n",
    "curseur = connexion.cursor()\n",
    "\n",
    "curseur.execute(\"\"\"DROP INDEX IF EXISTS idx_nom_gare_unique\"\"\")\n",
    "\n",
    "connexion.commit()\n",
    "connexion.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Requeter la base de données des objets trouvés pour récupérer les données entre 2019 et 2022 sur les gares parisiennes et Stocker les données dans une BDD SQL "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 40,
>>>>>>> 161ec014e927223b08a901b84e0eeb064c8605af
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "url = \"https://ressources.data.sncf.com/api/records/1.0/search/?dataset=objets-trouves-restitution&q=gc_obo_gare_origine_r_name+%3D+%22Paris%22+AND+(date%3D'2019'+OR+date%3D'2020')&sort=date&facet=date&facet=gc_obo_date_heure_restitution_c&facet=gc_obo_gare_origine_r_name&facet=gc_obo_nature_c&facet=gc_obo_type_c&facet=gc_obo_nom_recordtype_sc_c&timezone=Europe%2FParis&rows=-1\"\n",
=======
    "#url = \"https://ressources.data.sncf.com/api/records/1.0/search/?dataset=objets-trouves-restitution&q=gc_obo_gare_origine_r_name+%3D+%22Paris%22+AND+(date%3D'2019'+OR+date%3D'2020'+OR+date+%3D'2021'+OR+date%3D'2022')&sort=date&facet=date&facet=gc_obo_date_heure_restitution_c&facet=gc_obo_gare_origine_r_name&facet=gc_obo_nature_c&facet=gc_obo_type_c&facet=gc_obo_nom_recordtype_sc_c&timezone=Europe%2FParis&rows=-1\"\n",
>>>>>>> 161ec014e927223b08a901b84e0eeb064c8605af
    "\n",
    "url = \"https://ressources.data.sncf.com/api/records/1.0/search/?dataset=objets-trouves-restitution&q=gc_obo_gare_origine_r_name+%3D+%22Paris%22+AND+(date%3D'2019'+OR+date%3D'2020'+OR+date+%3D'2021'+OR+date%3D'2022')&sort=date&facet=date&facet=gc_obo_date_heure_restitution_c&facet=gc_obo_gare_origine_r_name&facet=gc_obo_nature_c&facet=gc_obo_type_c&facet=gc_obo_nom_recordtype_sc_c&timezone=Europe%2FParis&rows=-1\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "facet = data[\"parameters\"][\"facet\"]\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "# Extraire les valeurs de facet\n",
    "facets = facet\n",
    "date = facets[0]\n",
    "gc_obo_date_heure_restitution_c = facets[1]\n",
    "gc_obo_gare_origine_r_name = facets[2]\n",
    "gc_obo_nature_c = facets[3]\n",
    "gc_obo_type_c = facets[4]\n",
    "gc_obo_nom_recordtype_sc_c = facets[5]\n",
    "\n",
    "# Extraire les champs des enregistrements\n",
    "records = data[\"records\"]\n",
    "record_fields = []\n",
    "for record in records:\n",
    "    record_fields.append(record[\"fields\"])\n",
    "\n",
    "#print(record_fields)\n",
    "\n",
    "# Créer un DataFrame à partir des champs des enregistrements\n",
    "df = pd.DataFrame(record_fields)\n",
    "\n",
    "\n",
    "# Écrire le DataFrame dans un fichier CSV\n",
    "df.to_csv(\"objets-trouves-limite.csv\", index=False)\n",
    "\n",
    "\n",
    "# Créer une connexion à la base de données\n",
    "connexion = sqlite3.connect('bdd.db')\n",
    "\n",
    "df.to_sql('objets_trouves-limite', connexion,if_exists='replace', index=False)\n",
    "\n",
    "connexion.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requeter pour avoir une base de données complète"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "url1 = \"https://ressources.data.sncf.com/api/records/1.0/search/?dataset=objets-trouves-restitution&q=gc_obo_gare_origine_r_name+%3D+%22\"\n",
    "url2 = \"&sort=date&facet=date&facet=gc_obo_date_heure_restitution_c&facet=gc_obo_gare_origine_r_name&facet=gc_obo_nature_c&facet=gc_obo_type_c&facet=gc_obo_nom_recordtype_sc_c&timezone=Europe%2FParis&rows=-1\"\n",
    "\n",
    "\n",
    "gares = ['Paris Gare de Lyon', 'Paris Montparnasse', 'Paris Gare du Nord', 'Paris Saint-Lazare', 'Paris Est', 'Paris Bercy', 'Paris Austerlitz']\n",
    "date =[2019, 2020,2021,2022]\n",
    "\n",
    "data_frames = []\n",
    "for g in gares:\n",
    "    for d in date:\n",
    "        api_url = f\"{url1}{urllib.parse.quote(g)}%22+AND+date%3D{str(d)}{url2}\"\n",
    "        response = requests.get(api_url)\n",
    "        data = response.json()\n",
    "        records = data[\"records\"]\n",
    "        record_fields = [record[\"fields\"] for record in records]\n",
    "        df = pd.DataFrame(record_fields)\n",
    "        data_frames.append(df)\n",
    "\n",
    "df_concatenated = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "\n",
    "# Créer un DataFrame à partir des champs des enregistrements\n",
    "df = pd.DataFrame(df_concatenated)\n",
    "\n",
    "\n",
    "# Écrire le DataFrame dans un fichier CSV\n",
    "df.to_csv(\"objets-trouves.csv\", index=False)\n",
    "\n",
    "# Créer une connexion à la base de données\n",
    "connexion = sqlite3.connect('bdd.db')\n",
    "\n",
    "df.to_sql('objets_trouves', connexion,if_exists='replace', index=False)\n",
    "\n",
    "connexion.close()\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'API_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m weather_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCity\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTemperature\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFeels Like\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMin Temp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMax Temp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPressure\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mHumidity\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mWind Speed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mWind Direction\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSunrise\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSunset\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m gares:\n\u001b[0;32m---> 15\u001b[0m     api_url \u001b[39m=\u001b[39m url \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mappid=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m API_KEY \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m&q=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m city \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m&units=metric\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m#API CALL \u001b[39;00m\n\u001b[1;32m     16\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(api_url)   \u001b[39m#Get method\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'API_KEY' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "url = \"https://ressources.data.sncf.com/api/records/1.0/search/?dataset=objets-trouves-restitution&q=gc_obo_gare_origine_r_name+%3D+%22Paris%22+AND+(date%3D'2019'+OR+date%3D'2020'+OR+date+%3D'2021'+OR+date%3D'2022')&sort=date&facet=date&facet=gc_obo_date_heure_restitution_c&facet=gc_obo_gare_origine_r_name&facet=gc_obo_nature_c&facet=gc_obo_type_c&facet=gc_obo_nom_recordtype_sc_c&timezone=Europe%2FParis&rows=-1\"\n",
    "\n",
    "\n",
    "gares = ['Reims','Paris', 'Marseille', 'Lille', 'Rennes','Le Havre', 'Saint-Étienne', 'Toulon', 'Grenoble', 'Dijon', 'Angers', 'Nîmes', 'Villeurbanne','Lyon', 'Toulouse', 'Nice', 'Nantes', 'Strasbourg', 'Montpellier', 'Bordeaux']\n",
    "weather_data = pd.DataFrame(columns=[\"City\", \"Temperature\", \"Feels Like\", \"Min Temp\", \"Max Temp\", \"Pressure\", \"Humidity\", \"Wind Speed\", \"Wind Direction\", \"Sunrise\", \"Sunset\"])\n",
    "\n",
    "\n",
    "\n",
    "for g in gares:\n",
    "    api_url = url + \"appid=\" + API_KEY + \"&q=\" + city + \"&units=metric\" #API CALL \n",
    "    response = requests.get(api_url)   #Get method\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "facet = data[\"parameters\"][\"facet\"]\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "# Extraire les valeurs de facet\n",
    "facets = facet\n",
    "date = facets[0]\n",
    "gc_obo_date_heure_restitution_c = facets[1]\n",
    "gc_obo_gare_origine_r_name = facets[2]\n",
    "gc_obo_nature_c = facets[3]\n",
    "gc_obo_type_c = facets[4]\n",
    "gc_obo_nom_recordtype_sc_c = facets[5]\n",
    "\n",
    "# Extraire les champs des enregistrements\n",
    "records = data[\"records\"]\n",
    "record_fields = []\n",
    "for record in records:\n",
    "    record_fields.append(record[\"fields\"])\n",
    "\n",
    "#print(record_fields)\n",
    "\n",
    "# Créer un DataFrame à partir des champs des enregistrements\n",
    "df = pd.DataFrame(record_fields)\n",
    "\n",
    "\n",
    "# Écrire le DataFrame dans un fichier CSV\n",
    "df.to_csv(\"objets-trouves.csv\", index=False)\n",
    "\n",
    "\n",
    "# Créer une connexion à la base de données\n",
    "connexion = sqlite3.connect('bdd.db')\n",
    "\n",
    "df.to_sql('objets_trouves', connexion,if_exists='replace', index=False)\n",
    "\n",
    "connexion.close()"
   ]
  },
  {
=======
>>>>>>> 161ec014e927223b08a901b84e0eeb064c8605af
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Requeter la base de données de Logtitude et Latitude et Stocker les données dans une BDD SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "url_gares = \"https://ressources.data.sncf.com/api/records/1.0/search/?dataset=referentiel-gares-voyageurs&q=gare_alias_libelle_noncontraint='Paris'&sort=gare_alias_libelle_noncontraint&rows=-1\"\n",
    "\n",
    "\n",
    "response = requests.get(url_gares)\n",
    "\n",
    "gares_coord_list = []\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    for record in data['records']:\n",
    "        code_uic = record['fields']['uic_code']\n",
    "        gare_alias_libelle_noncontraint = record['fields']['gare_alias_libelle_noncontraint']\n",
    "        if 'wgs_84' in record['fields']:\n",
    "            latitude = record['fields']['wgs_84'][0]\n",
    "            longitude = record['fields']['wgs_84'][1]\n",
    "            gares_coord_list.append([code_uic,gare_alias_libelle_noncontraint, latitude, longitude])\n",
    "\n",
    "            \n",
    "else:\n",
    "    print(\"Une erreur s'est produite lors de la requête à l'API.\")\n",
    "    \n",
    "\n",
    "connexion = sqlite3.connect(\"base.db\")\n",
    "curseur = connexion.cursor()\n",
    "for gare in gares_coord_list:\n",
    "    code_uic = gare[0]\n",
    "    nom_gare = gare[1]\n",
    "    latitude = gare[2]\n",
    "    longitude = gare[3]\n",
    "    curseur.execute(\"INSERT INTO Gares (code_uic,nom_gare, latitude, longitude) VALUES (?,?, ?, ?)\", (code_uic, nom_gare, latitude, longitude))\n",
    "connexion.commit()\n",
    "connexion.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# url_gares = \"https://ressources.data.sncf.com/api/records/1.0/search/?dataset=referentiel-gares-voyageurs&q=&sort=gare_alias_libelle_noncontraint&rows=-1\"\n",
    "\n",
    "\n",
    "# response = requests.get(url_gares)\n",
    "\n",
    "# gares_coord_list = []\n",
    "\n",
    "# if response.status_code == 200:\n",
    "#     data = response.json()\n",
    "#     for record in data['records']:\n",
    "#         gare_alias_libelle_noncontraint = record['fields']['gare_alias_libelle_noncontraint']\n",
    "#         if 'wgs_84' in record['fields']:\n",
    "#             latitude = record['fields']['wgs_84'][0]\n",
    "#             longitude = record['fields']['wgs_84'][1]\n",
    "#             gares_coord_list.append([gare_alias_libelle_noncontraint, latitude, longitude])\n",
    "\n",
    "            \n",
    "# else:\n",
    "#     print(\"Une erreur s'est produite lors de la requête à l'API.\")\n",
    "    \n",
    "# df_data = pd.DataFrame(gares_coord_list, columns=[\n",
    "#     \"Gare\",\n",
    "#     \"Latitude\",\n",
    "#     \"Longitude\"\n",
    "# ])\n",
    "\n",
    "# df_data.to_csv('df_data.csv')\n",
    "\n",
    "# import sqlite3\n",
    "# import pandas as pd \n",
    "\n",
    "# connexion = sqlite3.connect('bdd.db')\n",
    "\n",
    "# df_data.to_sql('coordonnees_gares', connexion,if_exists='replace', index=False)\n",
    "\n",
    "# connexion.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Requêter la base de données de la fréquentation des gares et stocker les données dans une bdd SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import pandas as pd \n",
    "\n",
    "url_frequentation_gares = \"https://ressources.data.sncf.com/api/records/1.0/search/?dataset=frequentation-gares&q=nom_gare%3D%27Paris%27&sort=nom_gare&rows=-1\"\n",
    "\n",
    "\n",
    "response = requests.get(url_frequentation_gares)\n",
    "\n",
    "gare_frequentation_list = []\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    for record in data['records']:\n",
    "        gare = record['fields']['nom_gare']\n",
    "        frequentation_2019 = record['fields']['total_voyageurs_non_voyageurs_2019']\n",
    "        frequentation_2020 = record['fields']['total_voyageurs_non_voyageurs_2020']\n",
    "        frequentation_2021 = record['fields']['total_voyageurs_non_voyageurs_2021']\n",
    "        gare_frequentation_list.append([gare,frequentation_2019,frequentation_2020,frequentation_2021])\n",
    "        \n",
    "\n",
    "            \n",
    "else:\n",
    "    print(\"Une erreur s'est produite lors de la requête à l'API.\")\n",
    "    \n",
    "connexion = sqlite3.connect(\"base.db\")\n",
    "curseur = connexion.cursor()\n",
    "for gare in gare_frequentation_list:\n",
    "    nom_gare = gare[0]\n",
    "    frequentation_2019 = gare[1]\n",
    "    frequentation_2020 = gare[2]\n",
    "    frequentation_2021 = gare[3]\n",
    "    \n",
    "    curseur.execute(\"UPDATE Gares SET frequentation_2019= ?,frequentation_2020 = ?, frequentation_2021 = ? WHERE nom_gare=  ?\", (frequentation_2019, frequentation_2020, frequentation_2021,nom_gare))\n",
    "connexion.commit()\n",
    "connexion.close()\n",
    "\n",
    "connexion.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "connexion = sqlite3.connect(\"base.db\")\n",
    "curseur = connexion.cursor()\n",
    "curseur.execute(\"\"\"DELETE FROM Gares\n",
    "WHERE rowid NOT IN (\n",
    "  SELECT MIN(rowid)\n",
    "  FROM Gares\n",
    "  GROUP BY nom_gare, latitude, longitude\n",
    ")\n",
    "\"\"\")\n",
    "connexion.commit()\n",
    "connexion.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Requeter la base de données de la température et Stocker les données dans une BDD SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# Clé d'API de worldweatheronline\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY_TEMP = os.getenv(\"API_KEY_TEMP\")\n",
    "\n",
    "\n",
    "# Coordonnées de Paris\n",
    "COORDS = \"48.8566,2.3522\"\n",
    "\n",
    "# Définition de la période de temps\n",
    "start_date = datetime.date(2019, 1, 1)\n",
    "end_date = datetime.date(2022, 12, 31)\n",
    "delta = datetime.timedelta(days=35)\n",
    "\n",
    "# Liste pour stocker les données de température\n",
    "temperature_dates_list = []\n",
    "\n",
    "# Boucle pour récupérer les données de température par période de 35 jours\n",
    "while start_date <= end_date:\n",
    "    end_period = start_date + delta\n",
    "    if end_period > end_date:\n",
    "        end_period = end_date\n",
    "    url = f\"https://api.worldweatheronline.com/premium/v1/past-weather.ashx?q={COORDS}&date={start_date}&enddate={end_period}&tp=24&format=json&key={API_KEY_TEMP}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for weather in data['data']['weather']:\n",
    "            date = weather['date']\n",
    "            temperature_moyenne = weather['avgtempC']\n",
    "            temperature_dates_list.append([date,temperature_moyenne])\n",
    "    else:\n",
    "        print(\"Une erreur s'est produite lors de la requête à l'API.\")\n",
    "    start_date += delta\n",
    "\n",
    "for entree in temperature_dates_list:\n",
    "    gare = entree[0]\n",
    "    temperature = entree[1]\n",
    "    \n",
    "    curseur.execute(\"UPDATE Gares SET frequentation_2019= ?,frequentation_2020 = ?, frequentation_2021 = ? WHERE nom_gare=  ?\", (frequentation_2019, frequentation_2020, frequentation_2021,nom_gare))\n",
    "\n",
    "# Fermeture de la connexion à la base de données\n",
    "connexion.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce8263f54646eff212f75d029976341b1907292128b7f9fc9935844628a6029c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
